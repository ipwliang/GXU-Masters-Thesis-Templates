\section{基于CycleGAN网络改进的去雾还原算法\label{方法A}}

随着深度学习技术的飞速发展，生成对抗网络（GAN）及其变体在图像生成和转换领域取得了显著成果。去雾还原作为计算机视觉中的关键任务之一，旨在从有雾图像中恢复出清晰、无雾的图像，对于提高图像质量、增强视觉效果以及辅助后续的图像分析任务具有重要意义。CycleGAN \cite{cgan}作为一种新兴的无监督图像到图像转换方法，在去雾还原算法中展现出巨大潜力。

生成对抗网络（GAN）由 Goodfellow 等人于 2014 年提出\cite{gan}，其核心思想是通过两个神经网络——生成器（Generator）和判别器（Discriminator）的对抗训练来生成逼真的数据。生成器的目标是将随机噪声映射到目标数据分布，以生成尽可能真实的样本；判别器则负责区分生成样本和真实样本。在训练过程中，生成器和判别器不断竞争，生成器努力欺骗判别器，使其无法区分生成样本和真实样本，而判别器则努力提高自己的辨别能力。最终，理想情况下生成器能够生成以假乱真的样本，判别器则无法有效区分真假样本。

然而，GAN 在实际应用中存在一些局限性。首先，GAN 的训练过程较为复杂且不稳定，容易出现模式崩溃（Mode Collapse）问题，即生成器只能生成有限类型的样本，无法覆盖数据分布的多样性。其次，GAN 需要大量的成对数据进行训练，这在许多实际场景中难以获取，限制了其应用范围。

基于 CycleGAN 的改进去雾算法作为新一代无监督去雾方法，通过多尺度残差生成器设计、局部 - 全局判别器架构及空间自注意机制三大核心创新，显著提升了去雾图像的质量与真实性。本文基于改进 CycleGAN 网络，针对实际场景中因光照变化复杂、雾气类型多样及场景语义丰富导致的去雾效果不理想问题，提出针对性优化方案。本章将系统性阐述改进 CycleGAN 网络的核心架构设计及其在复杂场景去雾中的理论优势。

\subsection{CycleGAN网络结构分析}

\subsubsection{CycleGAN网络框架分析}

CycleGAN 是一种无监督的图像到图像转换方法，它在 2017 年由 Zhu 等人提出。CycleGAN 的核心思想是通过引入循环一致性约束（Cycle-Consistency Loss），在没有成对数据的情况下实现不同域之间的图像转换。CycleGAN 包含两个生成器（$G_A$ 和 $G_B$）和两个判别器（$D_A$ 和 $D_B$）。生成器 $G_A$ 的作用是将图像从域 $A$ 转换到域 $B$，而生成器 $G_B$ 则将图像从域 $B$ 转换回域 $A$。判别器 $D_A$ 和 $D_B$ 分别用于判断图像是否属于域 $A$ 和域$B$。

CycleGAN 的训练目标不仅包括对抗损失（Adversarial Loss），即生成器欺骗判别器的能力，还包括循环一致性损失。循环一致性损失确保图像经过两次转换后能够回到原始图像，从而保持图像内容的完整性。
具体来说，对于一张图像 $x$ 属于域 $A$，经过生成器 $G_A$ 转换到域 $B$ 得到图像 $G_A(x)$，再通过生成器 $G_B$ 转换回域 $A$ 得到图像 $x'$，循环一致性损失要求 $x'$ 尽可能接近原始图像 $x$；
同样地，对于一张图像 $y$ 属于域 $B$，经过生成器 $G_B$ 转换到域 $A$ 得到图像 $G_B(y)$，再通过生成器 $G_A$ 转换回域 $B$ 得到图像 $y'$，要求 $y'$ 尽可能接近原始图像 $y$。

% \begin{figure}[htb]
%     \centering
%     \subfloat{
%     \includegraphics[width=0.8\linewidth]{../figure/yolov11.png}
%     }
%     \captionsetup{font=footnotesize}
%     \bicaption{GAN 网络结构框架}{YOLOv11 network structure diagram.}
%     \label{fig:gan}
% \end{figure}

\begin{figure}[htb]
    \centering
    \subfloat{
    \includegraphics[width=0.8\linewidth]{../figure/cgan.png}
    }
    \captionsetup{font=footnotesize}
    \bicaption{CycleGAN 网络}{YOLOv11 network structure diagram.}
    \label{fig:cgan}
\end{figure}

GAN 通常需要大量的成对数据进行训练，这在许多实际应用中是一个巨大的挑战。例如，在去雾还原任务中，获取大量成对的有雾图像和对应的无雾图像是非常困难的，因为无雾图像往往难以直接获得，或者需要在特定条件下拍摄，这限制了 GAN 在该领域的应用。而 CycleGAN 则可以在无监督的情况下进行训练，即不需要成对的图像数据。它利用循环一致性约束，在只有两个不同域的非配对图像集合的情况下，学习图像之间的映射关系，这大大降低了数据获取的难度，为去雾还原等任务提供了更灵活的解决方案。

GAN 的网络结构相对简单，主要由一个生成器和一个判别器组成。生成器的目标是将随机噪声生成逼真的样本，判别器则用于区分生成样本和真实样本。GAN 的训练目标主要是通过最小化对抗损失函数，使生成器生成的样本尽可能接近真实数据分布，判别器尽可能准确地辨别真假样本。
CycleGAN 的网络结构更为复杂，包含两个生成器和两个判别器。其训练目标不仅包括对抗损失，还包括循环一致性损失。对抗损失确保生成器生成的图像在目标域中具有逼真的外观，而循环一致性损失则保证图像在转换过程中内容的完整性。这种双重约束机制使得 CycleGAN 能够在没有成对数据的情况下，学习到更稳定、更准确的图像转换映射关系。

\subsubsection{生成器网络}

%% 介绍 编码器 - 解码器 结构

CycleGAN是一种用于无监督图像到图像翻译的生成对抗网络架构。其生成器是整个网络的核心组件之一，主要承担着将输入图像从一个域（如雾天图像域）映射到目标域（如无雾图像域）的关键任务。生成器的目标是学习输入图像与目标图像之间的复杂非线性映射关系，使得生成的图像在视觉效果和语义信息上尽可能接近真实的目标域图像，从而实现图像的去雾还原等操作。

CycleGAN 的生成器通常采用编码器 - 解码器结构作为基础框架。编码器部分负责将输入图像逐步下采样，提取图像的多尺度特征。例如，在处理一张雾天图像时，编码器通过一系列卷积层和池化层操作，将图像的空间分辨率降低，同时增加特征图的通道数。这一过程能够捕捉到图像中的局部和全局特征，如雾天图像中的物体轮廓、颜色分布等信息。这些特征对于后续的图像生成至关重要，因为它们包含了生成无雾图像所需的关键语义信息。

解码器部分则与编码器相反，它将编码器提取到的特征逐步上采样，恢复图像的空间分辨率。在上采样过程中，解码器通过转置卷积等操作，将低分辨率的特征图逐步放大，最终生成与输入图像尺寸相同的输出图像。这个过程需要合理地利用编码器提取的特征，以确保生成的图像具有清晰的结构和细节。

\begin{figure}[htb]
    \centering
    \subfloat{
    \includegraphics[width=1.0\linewidth]{../figure/generator.png}
    }
    \captionsetup{font=footnotesize}
    \bicaption{生成器网络}{YOLOv11 network structure diagram.}
    \label{fig:gnet}
\end{figure}

%% 介绍ResNet残差块

为了提高生成器的性能，CycleGAN 在编码器 - 解码器结构中加入了残差块。残差块是深度残差网络（ResNet）中的重要组成部分，其核心思想是通过引入 shortcut connection（捷径连接）来缓解深层网络训练中的梯度消失问题。在生成器中，残差块使得网络能够更容易地学习到输入与输出之间的残差映射，而不是直接学习原始映射。

例如，在生成器的中间部分，多个残差块可以堆叠在一起。每个残差块包含两个卷积层和一个 ReLU 激活函数。输入特征先进入第一个卷积层进行卷积操作，然后通过 ReLU 激活函数引入非线性，接着进入第二个卷积层。最后，将第二个卷积层的输出与输入特征相加，得到残差块的输出。这种设计使得网络在训练过程中能够更有效地传播梯度，从而能够构建更深的生成器网络，以捕捉更复杂的图像特征和映射关系。

%% 卷积与反卷积

\begin{figure}[htb]
    \centering
    \subfloat{
    \includegraphics[width=0.7\linewidth]{../figure/conv_detail.png}
    }
    \captionsetup{font=footnotesize}
    \bicaption{卷积网络模块}{YOLOv11 network structure diagram.}
    \label{fig:conv_detail}
\end{figure}

\begin{figure}[htb]
    \centering
    \subfloat{
    \includegraphics[width=0.7\linewidth]{../figure/convtranspose.png}
    }
    \captionsetup{font=footnotesize}
    \bicaption{反卷积网络模块}{YOLOv11 network structure diagram.}
    \label{fig:convtranspose}
\end{figure}

卷积层是生成器中的基础组件，用于提取图像的局部特征。在编码器部分，卷积层通过卷积核在图像上滑动，进行加权求和操作，提取图像的边缘、纹理等特征。卷积层的输出通道数通常会逐渐增加，以捕捉更丰富的特征信息。

反卷积层（或转置卷积层）在解码器部分用于上采样操作，将低分辨率的特征图逐步放大。反卷积层通过学习卷积核的反向操作，将特征图的空间分辨率提高，从而恢复图像的细节。在反卷积过程中，合理的参数设置和初始化对于生成图像的质量至关重要。


\subsubsection{判别器网络}

全局判别器在整个图像层面进行判断，关注图像的整体特征和分布。它接收完整的图像作为输入，通过多层卷积操作提取图像的全局特征，包括图像的整体结构、颜色分布、纹理模式等宏观信息。其主要作用是判断输入图像是否符合目标域图像的整体特征分布，确保生成图像在整体上具有真实图像的外观和风格。例如，在去雾任务中，全局判别器会学习雾天图像和清晰图像在整体上的差异，如雾天图像通常具有较低的对比度、偏白的颜色倾向以及模糊的轮廓等特征，从而指导生成器生成具有清晰结构、自然颜色分布和高对比度的去雾图像。

全局判别器通常采用卷积神经网络结构，包含多个卷积层、激活函数层（如 Leaky ReLU）以及池化层。输入图像经过卷积层的逐层特征提取，图像的空间尺寸逐渐减小，而特征通道数逐渐增加，最终得到一个特征向量。该特征向量被送入全连接层，输出一个概率值，表示输入图像属于目标域真实图像的概率。在训练过程中，全局判别器与生成器进行对抗训练。生成器试图生成能够欺骗全局判别器的图像，使其误认为生成图像是真实图像；而全局判别器则不断学习如何更准确地区分真实图像和生成图像，通过梯度下降更新网络参数，优化判别性能。这种对抗训练过程促使生成器不断提升生成图像的整体质量，使其在全局特征上逐渐接近真实图像的分布。

\begin{figure}[htb]
    \centering
    \subfloat{
    \includegraphics[width=0.8\linewidth]{../figure/discriminator.png}
    }
    \captionsetup{font=footnotesize}
    \bicaption{判别器网络}{YOLOv11 network structure diagram.}
    \label{fig:discriminator}
\end{figure}

%% 全局判别器

全局判别器在去雾过程中发挥着关键作用。它能够确保去雾后的图像在整体上具有清晰、自然的视觉效果，避免出现整体结构失真或颜色偏差等问题。例如，当生成器生成的去雾图像整体对比度较低或颜色偏暗时，全局判别器会给出较低的真实性概率，从而促使生成器调整生成策略，提高去雾图像的整体质量。通过全局判别器的监督，生成器能够生成符合清晰图像全局特征的去雾结果，增强去雾图像的视觉吸引力和可用性。


局部判别器聚焦于图像的局部区域，关注图像的细节特征和局部纹理。它通过随机裁剪或采样等方法获取图像的局部小块区域作为输入，对这些局部区域进行判别。其主要作用是确保生成图像在局部细节上具有真实图像的纹理和结构特征，避免生成图像出现模糊、失真或不合理的局部图案。在去雾任务中，局部判别器能够帮助生成器恢复图像中被雾气遮挡的细节信息，如物体的边缘、纹理等，使去雾后的图像在局部区域更加清晰、真实。

局部判别器的网络结构与全局判别器类似，也是基于 CNN 构建。由于其输入是图像的局部小块区域，因此网络的层数和参数量相对较少。局部判别器对输入的局部区域进行卷积操作，提取局部特征，如边缘、纹理、局部颜色过渡等细节信息。经过多层卷积和激活函数处理后，输出一个概率值，表示该局部区域属于目标域真实图像对应局部区域的概率。在训练过程中，局部判别器同样与生成器进行对抗训练。生成器需要生成在局部细节上逼真的图像，以欺骗局部判别器；而局部判别器则不断学习如何更准确地辨别局部区域的真实性。这种对抗训练使得生成器在生成图像时更加注重局部细节的还原和真实感，提高了去雾图像的细节质量。

%% 局部判别器

局部判别器对于去雾效果的提升主要体现在图像细节的恢复和增强方面。在雾天图像中，物体的边缘和纹理往往被雾气模糊，细节信息丢失严重。局部判别器能够指导生成器在去雾过程中有效地恢复这些局部细节，使去雾后的图像能够呈现出物体清晰的轮廓和丰富的纹理。例如，对于建筑物的窗户、树叶的脉络等细节部位，局部判别器可以促使生成器生成更加精细、真实的细节，从而提高去雾图像的整体质量和视觉效果。同时，局部判别器还有助于避免生成图像出现局部失真或不合理的人工痕迹，确保去雾图像的自然性和可信度。


全局判别器和局部判别器在 CycleGAN 的去雾还原算法中相互补充、协同作用。全局判别器从整体上把握图像的宏观特征和风格，确保生成图像在整体布局、颜色分布等方面符合清晰图像的要求；局部判别器则专注于图像的细节部分，保证生成图像在局部区域具有真实的纹理和结构。在训练过程中，生成器需要同时满足全局判别器和局部判别器的要求，既要生成具有真实整体特征的图像，又要在局部细节上做到逼真自然。这种协同作用使得生成器能够生成高质量、高保真的去雾图像，有效地解决了图像去雾中的细节恢复和整体质量提升的难题。例如，在处理一幅包含复杂场景的雾天图像时，全局判别器确保整个场景的清晰度和自然度，如天空的蓝色、地面的颜色分布等；局部判别器则负责恢复场景中物体的细节，如人物的面部表情、车辆的标志等，从而使最终的去雾图像在整体和局部都达到较好的效果。

\subsection{优化的CycleGAN网络}

\subsubsection{Transformer模块}

随着深度学习的蓬勃发展，Transformer 架构在众多领域大放异彩。它最初在自然语言处理任务中取得突破性成果，如今也将其强大的学习能力拓展至图像处理领域。在基于 CycleGAN 的去雾还原算法中，将 Transformer 插入生成器网络，可显著提升对复杂去雾还原问题的学习效果，这源于 Transformer 独特且精妙的结构与机理。

Transformer 主要由编码器和解码器两部分构成，在一些图像相关任务的变体架构中，会根据实际需求对这两部分进行适当调整或简化。

%% Transformer

Transformer 的编码器通过多层堆叠结构（含多头自注意力机制与位置前馈网络子层），将输入数据转换为富含语义信息的中间表示。在去雾任务中，多头自注意力机制分析像素关联，把握图像结构；位置前馈网络提升模型表达力，助力学习复杂特征。
解码器结合编码器输出和目标序列先验信息生成去雾图像，其多层结构含掩码多头自注意力、多头注意力及位置前馈子层。掩码机制确保生成顺序性，多头注意力促进编码器与解码器间信息交互。
自注意力机制作为 Transformer 核心，通过查询、键、值向量计算元素关联权重，多头注意力并行运行多个自注意力头，增强模型表达与特征捕捉能力。位置编码（如正弦、余弦或可学习嵌入）弥补自注意力对位置信息的忽视，助力模型理解元素顺序。

本文提出一种基于 Transformer 改进的 Transformer 模块，巧妙地嵌入到基于 CycleGAN 的去雾还原算法的生成器网络中，显著增强了生成器对去雾还原任务的学习效能。

\begin{figure}[htb]
    \centering
    \subfloat{
    \includegraphics[width=1.0\linewidth]{../figure/transformer.png}
    }
    \captionsetup{font=footnotesize}
    \bicaption{改进的 Transformer 模块}{YOLOv11 network structure diagram.}
    \label{fig:transformer}
\end{figure}

改进的 Transformer 模块由多个核心组件协同构成。输入特征 $x$ 经历两次主要的加工循环。第一次循环聚焦于注意力机制部分：
先经由 LayerNorm 进行规范化，此处规范操作能稳固特征的数值分布；
随后输入到注意力模块中，实现特征序列中各元素间关联性的捕捉与衡量；
对输出特征实施重缩放和再偏置，这类操作有助于恢复特征的合理数值范围与分布特性，避免因规范化带来的信息畸变；
最后，将加工后的特征与原始输入特征相加，完成残差连接操作，残差连接可确保梯度顺畅传导，缓解深层网络训练时的梯度消失问题，保障模型稳定学习。

假设输入特征为 $X \in R^{H \times W \times C} $，其中 $H$ 为高度，$W$ 为宽度，$C$ 为通道数。
首先应用 LayerNorm 对输入进行归一化，可以表示为：
\begin{equation}
    \label{eq:transformer1}
    X_{norm},\ rescale,\ rebias = LayerNorm(X)
\end{equation}

公式\ref{eq:transformer1}中，$X_{norm}$ 是归一化后的特征，$rescale$ 和 $rebias$ 分别是归一化模块输出的缩放参数和偏置参数。

然后将归一化后的特征输入到注意力机制模块 attn：
\begin{equation}
    X_{attn} = attn(X_{norm})
\end{equation}

对注意力输出进行缩放和偏置调整：
\begin{equation}
    X_{attn\_scaled} = X_{attn} \times rescale + rebias
\end{equation}

最后与残差连接相加：
\begin{equation}
    X_{res} = X + X_{attn\_scaled}
\end{equation}

第二次循环则围绕多层感知机（mlp）展开：再次以输入特征为基准建立残差连接（identity）；
特征输入到 mlp 模块，历经两层线性变换搭配激活函数，实现对特征的非线性映射与维度拓展再压缩，深度挖掘特征的内在规律；
最终，将处理后的特征与本次循环起始的残差相加，输出本轮 Transformer 模块加工后的特征，为后续网络层传递更为丰富、抽象且贴合任务需求的特征表示。

将经过注意力机制和残差连接后的特征 $X_{res}$ 输入到多层感知机模块 mlp：
\begin{equation}
    X_{mlp} = mlp(X_{res})
\end{equation}

而多层感知机模块 mlp 由两个卷积层和一个激活函数组成：
\begin{equation}
    mlp(x) = Conv(ReLU(Conv(x)))
\end{equation}

将多层感知机的输出与残差连接相加得到最终输出：
\begin{equation}
    X_{out} = X_{res} + X_{mlp}
\end{equation}

相较于标准 Transformer 模块，该改进版融入了诸多契合视觉任务，尤其是去雾还原任务的创新设计。
其一，引入的窗口注意力机制及其移位策略，完美平衡了计算效率与感受野范围，在处理高维图像特征时避免了计算资源的过度消耗，同时确保模型能捕捉到图像全局结构与局部细节特征，这对于去雾任务中需要精准还原物体边缘、纹理等细节信息极为关键；
其二，灵活的规范化与残差连接配置选项，使整个模块能依据不同网络深度、特征维度场景，精准调控特征处理流程，提升模型适应性与性能表现，在 CycleGAN 生成器的复杂架构中，能与其他组件如卷积层、上采样层等无缝协同，全方位强化生成器对雾天图像特征的学习、转换与还原能力，最终输出高质量、视觉逼真的去雾图像成果。

综上所述，该改进的 Transformer 模块凭借精心设计的多头注意力机制、多层感知机模块，搭配巧妙的规范化与残差连接策略，深度契合去雾还原任务需求。将其嵌入 CycleGAN 生成器网络后，能显著增强模型对雾天图像复杂特征的学习、关联挖掘与高质量还原能力，推动基于 GAN 架构的图像去雾技术迈向新高度，为众多依赖清晰图像输入的下游计算机视觉应用提供更优质的视觉数据基础。

\subsubsection{改进的生成器网络}

CycleGAN 的生成器常借助 ResNet 残差链接结构来实现图像到图像的转换任务 。虽 ResNet 残差结构助力 CycleGAN 在诸多图像生成场景下有着不错表现，但面对一些对细节捕捉要求极高且数据复杂多样的情况，其基于传统卷积的特征提取与转换能力稍显不足，难以充分挖掘并利用图像深层复杂关联特征，一定程度上制约了生成图像的质量提升以及模型在更高端任务中的拓展应用。鉴于此，本文创新性地提出将 Transformer block 巧妙插入到 CycleGAN 生成器的 ResNet 残差块结构之中。

Transformer block 插入后的残差块结构如图 \ref{fig:cganformer} 
所示。该结构在保留 ResNet 残差核心优势基础上，融合了 Transformer 的强大特性，主要由自注意力机制模块、前馈神经网络模块以及与原 ResNet 残差结构的融合连接模块构成。自注意力机制模块能够捕捉图像不同位置间全局依赖关系，突破传统卷积局部感受野限制，使模型精准聚焦于图像关键信息区域，深度理解图像语义。前馈神经网络模块则进一步对特征进行非线性变换，增强特征表达丰富度。融合连接模块保障新结构与原 ResNet 残差架构无缝衔接，既继承残差学习优势，又充分释放 Transformer 潜能。

\begin{figure}[htb]
    \centering
    \subfloat{
    \includegraphics[width=1.0\linewidth]{../figure/cganformer.png}
    }
    \captionsetup{font=footnotesize}
    \bicaption{改进的生成器网络}{YOLOv11 network structure diagram.}
    \label{fig:cganformer}
\end{figure}

ResFormerGenerator 生成器网络由编码器、残差块、Transformer 块和解码器组成。其结构设计遵循了典型的编码 - 解码架构，能够有效地提取和还原图像的多尺度特征。

在 ResFormerGenerator 中，共有 6 个残差块。残差块的设计目的是解决深度神经网络中的梯度消失问题，并促进信息的高效传递。每个残差块包含两个卷积层和一个跳跃连接（skip connection）。跳跃连接将输入直接传递到块的输出，使得网络可以更容易地学习残差映射。

为了增强生成器网络对全局特征的学习能力，ResFormerGenerator 引入了 3 个改进的 Transformer 块。Transformer 块最初在自然语言处理领域取得巨大成功，其核心思想是利用自注意力机制捕捉序列中元素之间的长距离依赖关系。在图像处理中，可以将图像视为像素序列，从而应用 Transformer 块。
每个 Transformer 块包含多头自注意力和多层感知机。多头自注意力通过计算查询（Query）、键（Key）和值（Value）向量之间的点积，得到注意力权重矩阵，进而对输入特征进行加权求和，从而捕捉全局上下文信息。

编码器将输入的雾天图像特征进行编码，提取出主要的图像特征向量。假设输入的雾天图像特征为 $X_{in} \in R^{H \times W \times C}$，其中 $H$、$W$ 分别是图像的高度和宽度，$C$ 是通道数。编码器由一系列卷积层、归一化层和激活函数构成。经过编码器处理后，得到编码后的特征表示为 $F_{enc} \in R^{H' \times W' \times C'}$ ，其中 $H'$、$W'$、$C'$分别是编码后特征的高度、宽度和通道数。编码过程可以表示为：
\begin{equation}
    \label{eq:rfg1}
    F_{enc} = Encoder(X_{in})
\end{equation}

在 ResNet 块阶段，共有 6 个 ResNet 块。每个 ResNet 块包含两个卷积层、归一化层和激活函数，以及一个残差连接。对于第 $i$ 个 ResNet 块（$i$=1,2,...,6），其输入为 $F_{res_{i-1}}$，输出为 $F_{res_{i}}$ ​，处理过程如下：
\begin{equation}
    \label{eq:rfg2}
    F_{res_{i}} = ResNetBlock_i(F_{res_{i-1}})
\end{equation}

公式\ref{eq:rfg2}中，第 1 个 ResNet 块的输入为编码后的特征$F_{enc}$，即$F_{res_{0}}\ =\ F_{enc}$。经过6个ResNet块后，得到特征$F_{res_{6}}$。

在 Transformer 块阶段，共有 3 个 Transformer 块。对于第 $j$ 个 Transformer 块（$j$=1,2,3），其输入为 $F_{trans_{j-1}} + F_{res_{8-2j}}$，输出为 $F_{trans_{j}}$ ​，处理过程如下：
\begin{equation}
    \label{eq:rfg3}
    F_{trans_j} = TransformerBlock(F_{trans_{j-1}} + F_{res_{8-2j}}) 
\end{equation}

公式\ref{eq:rfg3}中，第 1 个 Transformer 块的输入为$F_{res_6}$。

解码器将经过 Transformer 块处理后的特征 $F_{trans_3}$ 进行上采样和卷积等操作，逐步恢复图像的空间分辨率。解码器由一系列反卷积层、归一化层和激活函数构成。最终输出去雾还原后的图像特征 $X_{out} \in R^{H \times W \times C_{out}}$，其中 $C_{out}$ 是通道数。解码过程可以表示为：
\begin{equation}
    \label{eq:rfg4}
    X_{out} = Decoder(F_{trans_3})
\end{equation}


Transformer Block 中的自注意力机制能够捕捉特征图中不同位置之间的全局依赖关系，这对于理解雾气在图像中的分布具有重要意义。相比之下，传统的卷积操作由于其局部感受野的限制，难以有效捕捉全局信息。通过引入自注意力机制，模型能够更好地学习到雾气在图像中的分布规律，从而提升去雾效果。

在前向传播过程中，跳跃连接将前面残差块的输出与当前 Transformer 块的输出相加，这种设计能够更好地融合不同层次的特征信息。低层特征包含丰富的纹理细节，而高层特征则包含语义信息。通过跳跃连接，模型能够综合利用不同层次的特征，生成更清晰、更自然的去雾图像。

\subsection{图像去雾实验和雾天目标检测实验结果和分析}

新算法 CGANFormer 在 NYU2\cite{nyu2}、Dense-Haze\cite{NTIRE_Dehazing_2019} 数据集上进行训练和测试图像去雾性能，以改进每个阶段。
在不同场景中选择复杂的场景图片，并将拟议算法的检测效果与实际场景中，并与 AODNet\cite{li2017aod}、FFA-Net\cite{ffa}、GCANet\cite{chen2019gated} 和 CycleGAN\cite{cgan} 算法进行比较。

最后将 CGANFormer 算法与 EX-YOLO算法结合，在FOG-TT100K、FOG-VisDrone数据集上进行雾天目标检测性能测试，并与YOLOv9s\cite{yolov9}、YOLOv10s\cite{yolov10}、YOLOv11s\cite{yolov11}算法进行比较。

\subsubsection{数据集}

NYU2 数据集是室内场景视觉研究领域的重要数据集，由纽约大学（NYU）构建。它包含 1449 对密集标注的 RGB 和深度图像，这些图像从多个城市的 464 个新场景中采集，利用微软 Kinect 的 RGB 和深度摄像机记录视频序列，还包含 407024 个新的未标注帧，为图像去雾等研究提供了大量室内场景的视觉数据资源，可用于训练和评估去雾算法在复杂室内环境中的性能。

Dense-Haze 是一个用于图像去雾研究的高质量数据集，由 CVL 于 2019 年为 NTIRE 图像去雾挑战赛构建。该数据集包含 33 对真实雾气和无雾图像对，涵盖多样化的户外场景。这些图像对是在相同照明参数下，通过专业设备引入真实雾气生成技术拍摄的，具有高密度、均匀分布的雾气特征，旨在推动单图像去雾技术的发展。Dense-Haze 数据集不仅为去雾算法提供了可靠的评估基准，还通过综合定性和定量评估揭示了现有方法在处理密集均匀雾气场景时的局限性，为未来研究指明了改进方向。


\subsubsection{实验环境}

本文中用于实验的系统、系统硬件设施和软件平台如表\ref{tab:environment2}所示。
\begin{table}[htbp]
    \centering
    \captionsetup{font=footnotesize}
    \bicaption{实验环境设置}{Symbol cross-reference table}
    \label{tab:environment2}
    \begin{tabular}{>{\centering\arraybackslash}p{0.4\textwidth}>{\centering\arraybackslash}p{0.4\textwidth}}
        \toprule
        List              & Version            \\ 
        \midrule
        Operating System  & Ubuntu 22          \\
        Memory            & 64G RAM            \\
        CPU               & Intel i9-13900K    \\
        GPU               & NVIDIA GTX4090 GPU \\
        Cuda              & cu121              \\
        Python            & 3.11               \\
        \bottomrule
    \end{tabular}
\end{table}

\subsubsection{实验结果和分析}

在这项研究中，我们通过比较实验评估了 CGANFormer 模型（our）在2个不同数据集中，包括 NYU2 和 Dense-Haze 数据集上的性能，以验证图像去雾还原的改进方法的有效性。
通过将其与基准模型 CycleGAN、AODNet、FFA-Net、GCANet 进行比较，评估改进模型在各种指标上的性能。

\begin{table}[htbp]
    \centering
    \captionsetup{font=footnotesize}
    \bicaption{在 NYU2 数据集上的对比实验结果}{Symbol cross-reference table}
    \label{tab:compare_studies_nyu2}
    \begin{tabular}{p{0.13\textwidth}p{0.13\textwidth}p{0.13\textwidth}p{0.13\textwidth}p{0.13\textwidth}p{0.13\textwidth}}
        \toprule
        性能指标 & CycleGAN & AODNet & FFA-Net & GCANet & our   \\ 
        \midrule
        PSNR    & 9.5      & 21.8   & 0.877   & 0.878  & 0.777 \\
        SSIM    & 8.2      & 25.4   & 0.850   & 0.776  & 0.752 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[htbp]
    \centering
    \captionsetup{font=footnotesize}
    \bicaption{在 Dense-Haze 数据集上的对比实验结果}{Symbol cross-reference table}
    \label{tab:compare_studies_dh}
    \begin{tabular}{p{0.13\textwidth}p{0.13\textwidth}p{0.13\textwidth}p{0.13\textwidth}p{0.13\textwidth}p{0.13\textwidth}}
        \toprule
        性能指标 & CycleGAN & AODNet & FFA-Net & GCANet & our   \\ 
        \midrule
        PSNR    & 9.5      & 21.8   & 0.877   & 0.878  & 0.777 \\
        SSIM    & 8.2      & 25.4   & 0.850   & 0.776  & 0.752 \\
        \bottomrule
    \end{tabular}
\end{table}


接下来比较我们通过比较实验评估了EX-YOLO模型（our）在2个不同数据集中，包括TT100K和VisDrone数据集上的性能，以验证小目标检测和多尺度目标检测的改进方法的有效性。
通过将其与基准模型YOLOv11s、YOLOv10s和YOLOv9s进行比较，评估改进模型在各种指标上的性能。

\begin{table}[htbp]
    \centering
    \captionsetup{font=footnotesize}
    \bicaption{在 FOG-TT100K 数据集上的对比实验结果}{Symbol cross-reference table}
    \label{tab:compare_studies_fogtt100k}
    \begin{tabular}{p{0.13\textwidth}p{0.13\textwidth}p{0.19\textwidth}p{0.1\textwidth}p{0.07\textwidth}p{0.07\textwidth}p{0.07\textwidth}}
        \toprule
        模型       & 参数量 MB & 计算量 GFLOPs & $mAP_{0.5}$   & P     & R     & FPS \\ 
        \midrule
        YOLOv11s     & 9.5           & 21.8          & 0.877          & 0.878  & 0.777 & 94.3 \\
        YOLOv10s     & 8.2           & 25.4          & 0.850          & 0.776  & 0.752 & 88.5 \\
        YOLOv9s      & 7.5           & 27.2          & \textbf{0.906} & 0.914  & 0.818 & 90.9 \\
        \textbf{our} & 11.4          & \textbf{17.0} & 0.893          & 0.875  & 0.830 & 84.7 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[htbp]
    \centering
        \subfloat[EX-YOLO\label{fig:fogtt100k_ex_cmn}]{
            \includegraphics[width=0.4\textwidth]{../figure/tt100k_ex_confusion_matrix_normalized.png}
        }
        \subfloat[YOLOv11s\label{fig:fogtt100k_11s_cmn}]{
            \includegraphics[width=0.4\textwidth]{../figure/tt100k_v11s_confusion_matrix_normalized.png}
        } \\
        \subfloat[YOLOv10s\label{fig:fogtt100k_10s_cmn}]{
            \includegraphics[width=0.4\textwidth]{../figure/tt100k_v10s_confusion_matrix_normalized.png}
        }
        \subfloat[YOLOv9s\label{fig:fogtt100k_9s_cmn}]{
            \includegraphics[width=0.4\textwidth]{../figure/tt100k_v9s_confusion_matrix_normalized.png}
        }
    \captionsetup{font=footnotesize}
    \bicaption{不同的网络模型在 FOG-TT100K 数据集上的归一化混淆矩阵}{Symbol cross-reference table}
    \label{fig:fogt100k_cmn}
\end{figure}

\begin{figure}[htbp]
    \centering
        \subfloat[EX-YOLO\label{fig:fogtt100k_ex_f1}]{
            \includegraphics[width=0.4\textwidth]{../figure/tt100k_ex_F1_curve.png}
        }
        \subfloat[YOLOv11s\label{fig:fogtt100k_11s_f1}]{
            \includegraphics[width=0.4\textwidth]{../figure/tt100k_v11s_F1_curve.png}
        } \\
        \subfloat[YOLOv10s\label{fig:fogtt100k_10s_f1}]{
            \includegraphics[width=0.4\textwidth]{../figure/tt100k_v10s_F1_curve.png}
        }
        \subfloat[YOLOv9s\label{fig:fogtt100k_9s_f1}]{
            \includegraphics[width=0.4\textwidth]{../figure/tt100k_v9s_F1_curve.png}
        }
    \captionsetup{font=footnotesize}
    \bicaption{不同的网络模型在 FOG-TT100K 数据集上的F1得分}{Symbol cross-reference table}
    \label{fig:fogtt100k_f1}
\end{figure}


\begin{table}[htbp]
    \centering
    \captionsetup{font=footnotesize}
    \bicaption{在 FOG-VisDrone 数据集上的对比实验结果}{Symbol cross-reference table}
    \label{tab:compare_studies_fogvd}
    \begin{tabular}{p{0.13\textwidth}p{0.13\textwidth}p{0.19\textwidth}p{0.1\textwidth}p{0.07\textwidth}p{0.07\textwidth}p{0.07\textwidth}}
        \toprule
        模型       & 参数量 MB & 计算量 GFLOPs & $mAP_{0.5}$   & P     & R     & FPS \\ 
        \midrule
        YOLOv11s     & 9.4   & 21.3         & 0.383           & 0.485  & 0.381 & 82.6 \\
        YOLOv10s     & 8.0   & 24.5         & 0.390           & 0.501  & 0.380 & 78.1 \\
        YOLOv9s      & 7.2   & 26.7         & \textbf{0.402}           & 0.525  & 0.392 & 62.1 \\
        \textbf{our} & 11.4  & \textbf{15.4} & 0.390 & 0.514  & 0.372 & 68.5 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}[htbp]
    \centering
        \subfloat[EX-YOLO\label{fig:fogvd_ex_cmn}]{
            \includegraphics[width=0.4\textwidth]{../figure/vd_ex_confusion_matrix_normalized.png}
        }
        \subfloat[YOLOv11s\label{fig:fogvd_11s_cmn}]{
            \includegraphics[width=0.4\textwidth]{../figure/vd_v11s_confusion_matrix_normalized.png}
        } \\
        \subfloat[YOLOv10s\label{fig:fogvd_10s_cmn}]{
            \includegraphics[width=0.4\textwidth]{../figure/vd_v10s_confusion_matrix_normalized.png}
        }
        \subfloat[YOLOv9s\label{fig:fogvd_9s_cmn}]{
            \includegraphics[width=0.4\textwidth]{../figure/vd_v9s_confusion_matrix_normalized.png}
        }
    \captionsetup{font=footnotesize}
    \bicaption{不同的网络模型在 FOG-VisDrone 数据集上的归一化混淆矩阵}{Symbol cross-reference table}
    \label{fig:fogvd_cmn}
\end{figure}

\begin{figure}[htbp]
    \centering
    \subfloat[EX-YOLO\label{fig:fogvd_ex_f1}]{
        \includegraphics[width=0.4\textwidth]{../figure/vd_ex_F1_curve.png}
    }
    \subfloat[YOLOv11s\label{fig:fogvd_11s_f1}]{
        \includegraphics[width=0.4\textwidth]{../figure/vd_v11s_F1_curve.png}
    } \\
    \subfloat[YOLOv10s\label{fig:fogvd_10s_f1}]{
        \includegraphics[width=0.4\textwidth]{../figure/vd_v10s_F1_curve.png}
    }
    \subfloat[YOLOv9s\label{fig:fogvd_9s_f1}]{
        \includegraphics[width=0.4\textwidth]{../figure/vd_v9s_F1_curve.png}
    }
    \captionsetup{font=footnotesize}
    \bicaption{不同的网络模型在 FOG-VidDrone 数据集上的F1得分}{Symbol cross-reference table}
    \label{fig:fogvd_f1}
\end{figure}

\subsection{本章小结}

